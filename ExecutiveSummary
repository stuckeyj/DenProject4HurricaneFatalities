Executive Summary- Project 4
  Julia Taussig, Drey Dyson, & Jorian Stuckey
  April 2019

The problem.
  Which factors associated with hurricanes have the highest correlation with fatality rates? How could we use this information to positively impact survival rates?

The data.
  NOAA's hurricane center is the authority on hurricane storm data (HURDAT). Once a storm is observed, measurements are taken once every 6 hours until the storm dissipates or is absorbed into another weather pattern. HURDAT includes the wind speed, wind speed information for each quadrant of the storm, the pressure of the storm, and the latitude and longitude of each measurement. For the purposes of this project, we only looked at Atlantic storms that occurred between 1969-2017 and had some measurable impact on the United States (in terms of damages or fatalities).
    -HURDAT data is inconsistent prior to 2004, and of the 303 storms on our initial list, 123 were missing nearly all of the meteorological data. In hindsight, we would have only modeled using data from storms starting in 2004, and while that left many fewer data points to work with, it would have provided many more features within each row.
    -We built a simplified dataset with one row per storm where we took either the first landfall instance or the first measured incidence of the storm. This was done in order to expedite modeling and to assist in creating basic visualizations including number or storms by year and month, landfall locations when applicable, etc.

  To gather fatality numbers by state and county, we used NOAA's Storm Data by Year by Month datasets.
    -Many of these documents lacked the county name for each fatality. If we could find that information on Wikipedia or any other source, it was added to the dataframe. If not, it was left out. Information became less reliable (and harder to read) the further back we went.
    -Total fatality data by state was added to the simplified HURDAT dataframe.

  Once we had our list of fatalities and locations, we used US Census data and information from USGS to match counties up with GIS coordinates.

Exploration.
  Once we combined the data, we used Tableau to create visualizations to help us better understand it. It wasn't until we pulled the fatalities by county data into Tableau that we determined that the original data source we had used to match county with latitude and longitude was flawed and that the  dataframe needed to be rebuilt.

  We tried using a dictionary to link the lat & long with county information, but switched to using k-nearest neighbors because of issues with open water not having a county.

  We ended up creating and using several separate dataframes for this project because of issues with lining up the time information in the HURDAT set with the location information in the fatalities dataset. We remain unsure of the best way to possible combine them, if it exists.


  Unsurprisingly, Katrina (2005) was a huge outlier, and the sheer volume of fatalities made it challenging to determine a county for each. Many many people remain missing to this day.


Modeling.
  After spending so much time wrestling with different aspects of our data, reaching the modeling stage was quite exciting! Julia set out to create a model to predict fatalities, and tried just about every model we've gone over in class so far(linear regression, ). Despite engineering a feature for time elapsed since the last hurricane and meticulously tweaking her models, she was quite disappointed with her results. Her most successful model was the K-nearest-neighbors, with a train score of 0.45, a test score of 0.21, and a CV score of 0.16. Her baseline prediction was 7 deaths per hurricane (the average without Katrina). The baseline with Katrina included was 14 deaths per storm. Needless to say, Katrina had to be dropped often because of her effect on our results.

  After battling it out with the remarkably difficult latitude and longitude data, Drew used a logistic regression on that data to predict storm status (hurricane, tropical storm, tropical depression, etc). He was quite successful with a train score of 0.837 and a test score of 0.825.

Evaluation.
  Can we create a model to predict the number of fatalities caused by a given hurricane? Sure. Can we do it well? Only to a point.

Conclusions.
  Because of the lack of complete meteorological data pre-2004, we are sorely lacking in information to make a truly informed model to predict fatalities. As the years continue on, the models will get better as the dataframe becomes more robust. In the meantime, we believe that more data can be gathered elsewhere, including storm surge data, rainfall information by county, detailed population information (nearly 50% of Katrina fatalities were seniors), etc.

  All that said, many of the deaths supposedly caused by hurricanes are more attributable to human error. Even if we generate an incredible model that can tell us exactly how deadly a storm is likely to be and thus when/where to order an evacuation, we probably won't stop hurricane parties, keep surfers out of the water, prevent stubborn drivers from detouring around police barricades, etc. Even if we could remedy humans being human, there's still millions of people in hurricane-prone areas that are unable to evacuate for any of several reasons.

Challenges:
  Innumerable- if it could go wrong, it did! HURDAT was a bear to understand, but we finally got there with the help of Stephen Jaye and Sam Stack. Initially we planned on using population data that started in 1969, hence the removal of the rest of the HURDAT data. That didn't work out, but we decided to just keep moving. Gathering the fatality data from old PDF files was tedious and time-consuming. The initial dataset we used for lat and log data was incorrect and had to be resourced. Tableau didn't like exporting more than one map image at a time. You name it, we dealt with it. As with any other new experience, we learned a lot that will hopefully serve us well in our future endeavors.

Further Areas of Research:
  Also innumerable! We would like to circle back and look at the data that is missing from what we have, and add to it in order to strengthen our models. We would like to explore more of the mechanics behind evacuation orders as each state has their own rules. Perhaps we could use state of emergency declarations as a feature in an updated model somehow. There is so much here that there is always more to do.
